{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1jZ0fHMoumXsZfjEYqCISI_DbI4_R3LFQ",
      "authorship_tag": "ABX9TyPExfbBSfhPcecJE2zCUgIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SergioCarmo-ro/PROJETO-MONITORAMENTO-EM-TEMPO-REAL_PYTHOCH/blob/main/pythoch_aluno_tempo_real4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROJETO MONITORAMENTO EM TEMPO REAL_PYTHOCH\n"
      ],
      "metadata": {
        "id": "V4_9Id85eLc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as TF\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18, vgg16\n",
        "from torchvision.models.resnet import ResNet18_Weights\n",
        "from torchvision.models.vgg import VGG16_Weights\n",
        "import copy\n",
        "import numpy as np\n",
        "import random\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "LdwgzYLSeT8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc002ece"
      },
      "source": [
        "def set_seed(seed=None, seed_torch=True):\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if seed_torch:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "    print(f'Random seed {seed} has been set.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def show_triplets(dataloader, n=5):\n",
        "  data_iter = iter(dataloader)\n",
        "  shown = 0\n",
        "  plt.figure(figsize=(12, 4 * n))\n",
        "  while shown < n:\n",
        "        try:\n",
        "            anchors, positives, negatives = next(data_iter)\n",
        "        except StopIteration:\n",
        "            print(\"End of DataLoader reached.\")\n",
        "            break\n",
        "\n",
        "        for i in range(anchors.size(0)):\n",
        "            if shown >= n:\n",
        "                break\n",
        "\n",
        "            for j, img in enumerate([anchors[i], positives[i], negatives[i]]):\n",
        "                img = img.squeeze(0) if img.shape[0] == 1 else img  # Remove channel dim if grayscale\n",
        "                plt.subplot(n, 3, shown * 3 + j + 1)\n",
        "                plt.imshow(img.numpy(), cmap='gray')\n",
        "                if j == 0:\n",
        "                    plt.title(\"Anchor\")\n",
        "                elif j == 1:\n",
        "                    plt.title(\"Positive\")\n",
        "                else:\n",
        "                    plt.title(\"Negative\")\n",
        "                plt.axis('off')\n",
        "\n",
        "            shown += 1\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "def plot_loss_curve(train_loss, val_loss):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_loss, label='Train Loss', marker='o')\n",
        "    plt.plot(val_loss, label='Validation Loss', marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Triplet Loss Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy_curve(train_acc, val_acc):\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(train_acc, label='Train Accuracy', marker='o')\n",
        "    plt.plot(val_acc, label='Validation Accuracy', marker='o')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Triplet Embedding Accuracy Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "XrsNhBp9eXTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6t29uCVdyIv"
      },
      "outputs": [],
      "source": [
        "class TripletVGGFaceDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, max_samples=None):\n",
        "\n",
        "       # Argumentos:\n",
        "        # root_dir (str): Caminho para o diretório 'train/' ou 'val/'.\n",
        "        # transform: transformações do Torchvision a serem aplicadas.\n",
        "        # max_samples (int): Comprimento do conjunto de dados (padrão: 10000).\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.max_samples = max_samples or 10000\n",
        "\n",
        "        self.class_dirs = [d for d in os.listdir(root_dir)\n",
        "                           if os.path.isdir(os.path.join(root_dir, d))]\n",
        "\n",
        "        self.image_paths = {\n",
        "            cls: [os.path.join(root_dir, cls, f)\n",
        "                  for f in os.listdir(os.path.join(root_dir, cls))\n",
        "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "            for cls in self.class_dirs\n",
        "        }\n",
        "\n",
        "        # Mantenha apenas classes com >=2 imagens (necessário para trigêmeos)\n",
        "        self.class_dirs = [cls for cls in self.class_dirs if len(self.image_paths[cls]) >= 2]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.max_samples\n",
        "\n",
        "    def _load_gray_image(self, img_path):\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is None:\n",
        "            raise ValueError(f\"Could not read image: {img_path}\")\n",
        "\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        gray_pil = Image.fromarray(gray, mode='L')\n",
        "        return gray_pil\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        while True:\n",
        "            try:\n",
        "                anchor_cls = random.choice(self.class_dirs)\n",
        "                anchor_img_path, positive_img_path = random.sample(self.image_paths[anchor_cls], 2)\n",
        "\n",
        "                negative_cls = random.choice(self.class_dirs)\n",
        "                while negative_cls == anchor_cls:\n",
        "                    negative_cls = random.choice(self.class_dirs)\n",
        "                negative_img_path = random.choice(self.image_paths[negative_cls])\n",
        "\n",
        "                anchor = self._load_gray_image(anchor_img_path)\n",
        "                positive = self._load_gray_image(positive_img_path)\n",
        "                negative = self._load_gray_image(negative_img_path)\n",
        "\n",
        "                if self.transform:\n",
        "                    anchor = self.transform(anchor)\n",
        "                    positive = self.transform(positive)\n",
        "                    negative = self.transform(negative)\n",
        "\n",
        "                return anchor, positive, negative\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping triplet: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataset = TripletVGGFaceDataset(\n",
        "    root_dir='/content/drive/MyDrive/Alunos',\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "CpZw6PUynGHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "#show_triplets(train_loader, n=4)"
      ],
      "metadata": {
        "id": "BvbJ7u-nnaxK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    # Envolva qualquer modelo de backbone e o adapte para gerar uma incorporação normalizada.\n",
        "\n",
        "    def __init__(self, backbone, embedding_dim=128):\n",
        "\n",
        "      # Argumentos:\n",
        "      #   backbone (nn.Module): Modelo de backbone com um extrator de recursos.\n",
        "      #   embedding_dim (int): Tamanho da incorporação de saída.\n",
        "\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        # Remover classificador do backbone (assume o formato padrão do torchvision)\n",
        "        if hasattr(backbone, 'fc'):  # Para modelos do tipo ResNet\n",
        "            self.features = nn.Sequential(*list(backbone.children())[:-1])\n",
        "            in_features = backbone.fc.in_features\n",
        "        elif hasattr(backbone, 'classifier'):  # Para MobileNet, etc.\n",
        "            self.features = nn.Sequential(*list(backbone.children())[:-1])\n",
        "            in_features = backbone.classifier[1].in_features\n",
        "        else:\n",
        "            raise ValueError(\"Arquitetura de backbone sem suporte\")\n",
        "\n",
        "        # Nova camada de incorporação\n",
        "\n",
        "        self.embedding = nn.Linear(in_features, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Passagem de encaminhamento para extrair e normalizar embeddings.\n",
        "\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.embedding(x)\n",
        "        return F.normalize(x, p=2, dim=1)  # L2 normalization\n",
        "\n",
        "def load_backbone(model_name=\"resnet18\", pretrained=True):\n",
        "\n",
        "    # Carregar dinamicamente a arquitetura do backbone com pesos pré-treinados opcionais.\n",
        "\n",
        "    # Argumentos:\n",
        "    # model_name (str): Arquitetura do backbone (ex.: 'resnet18').\n",
        "    # pretrained (bool): Se deve carregar pesos pré-treinados do ImageNet.\n",
        "\n",
        "    # Retorna:\n",
        "    # nn.Module: Modelo do backbone\n",
        "\n",
        "    if model_name == \"resnet18\":\n",
        "        weights = ResNet18_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        model = resnet18(weights=weights)\n",
        "\n",
        "        # Substituir a primeira camada de conversão por tons de cinza (1 canal)\n",
        "        model.conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=64,\n",
        "            kernel_size=7,\n",
        "            stride=2,\n",
        "            padding=3,\n",
        "            bias=False\n",
        "        )\n",
        "        return model\n",
        "    elif model_name == \"resnet50\":\n",
        "        weights = ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        model = resnet50(weights=weights)\n",
        "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        return model\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Backbone '{model_name}' ainda não é suportado.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NWVzCtDInf9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TripletNetwork(nn.Module):\n",
        "\n",
        "    #Wrapper de rede tripleto que gera embeddings de um modelo de backbone.\n",
        "\n",
        "    def __init__(self, model_name=\"resnet18\", embedding_dim=128, pretrained=True):\n",
        "\n",
        "        # Argumentos:\n",
        "          # model_name (str): Nome do modelo de backbone.\n",
        "          # embedding_dim (int): Tamanho da incorporação de recursos de saída.\n",
        "          # pretrained (bool): Usar pesos pré-treinados ou não.\n",
        "\n",
        "        super(TripletNetwork, self).__init__()\n",
        "\n",
        "        # Carregue o backbone e envolva-o em um extrator de recursos\n",
        "        backbone = load_backbone(model_name, pretrained)\n",
        "        self.embedding_model = FeatureExtractor(backbone, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "       # Passagem direta pela rede de incorporação.\n",
        "\n",
        "        return self.embedding_model(x)"
      ],
      "metadata": {
        "id": "ZhvFFYUqooxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_triplet_model(weights_path, model_name=\"resnet18\", embedding_dim=128, pretrained=True, device=None):\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = TripletNetwork(model_name=model_name, embedding_dim=embedding_dim, pretrained=pretrained)\n",
        "    state_dict = torch.load(weights_path, map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    return model\n",
        "\n",
        "# ---- Wrapper do Classificador ----\n",
        "\n",
        "\n",
        "class TripletClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 weights_path,\n",
        "                 num_classes,\n",
        "                 embedding_dim=128,\n",
        "                 freeze_until=None,\n",
        "                 model_name=\"resnet18\",\n",
        "                 pretrained=True,\n",
        "                 device=None):\n",
        "\n",
        "      # Argumentos:\n",
        "        # weights_path (str): Caminho para os pesos salvos do TripletNetwork.\n",
        "        # num_classes (int): Número de classes de saída.\n",
        "        # embedding_dim (int): Dimensionalidade da camada de incorporação.\n",
        "        # freeze_until (int ou None): Índice da camada no ResNet18 até o congelamento.\n",
        "        # model_name (str): Nome da arquitetura do backbone.\n",
        "        # pretrained (bool): Se o backbone deve ser inicializado com pesos do ImageNet.\n",
        "        # device (str ou torch.device): Dispositivo de destino.\n",
        "\n",
        "        super(TripletClassifier, self).__init__()\n",
        "\n",
        "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Carregue o TripletNetwork usando a função auxiliar\n",
        "        self.backbone = load_triplet_model(\n",
        "            weights_path=weights_path,\n",
        "            model_name=model_name,\n",
        "            embedding_dim=embedding_dim,\n",
        "            pretrained=pretrained,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        # Congelar opcionalmente parte do backbone\n",
        "        if freeze_until is not None:\n",
        "            self.freeze_layers(freeze_until)\n",
        "\n",
        "        # Adicionar uma cabeça de classificador\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def freeze_layers(self, freeze_until):\n",
        "        modules = list(self.backbone.embedding_model.features.children())\n",
        "        for i, layer in enumerate(modules):\n",
        "            if i < freeze_until:\n",
        "                for param in layer.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.backbone(x)\n",
        "        logits = self.classifier(embedding)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Nu_jw5KbuJiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def compute_accuracy(anchor, positive, negative):\n",
        "    pos_dist = F.pairwise_distance(anchor, positive, p=2)\n",
        "    neg_dist = F.pairwise_distance(anchor, negative, p=2)\n",
        "    correct = (pos_dist < neg_dist).sum().item()\n",
        "    total = anchor.size(0)\n",
        "    return correct, total"
      ],
      "metadata": {
        "id": "Um89euSkvhpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_triplet_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, save_path='best_model.pth', use_tqdm=True, scheduler=None):\n",
        "    model = model.to(device)\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_acc': [],\n",
        "        'train_f1': [],\n",
        "        'val_f1': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}\\n' + '-'*30)\n",
        "\n",
        "        # -------- Train --------\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        train_preds = []\n",
        "        train_labels = []\n",
        "\n",
        "        if use_tqdm:\n",
        "            train_loader = tqdm(train_loader, desc='training')\n",
        "\n",
        "        for anchor, positive, negative in train_loader:\n",
        "            anchor = anchor.to(device)\n",
        "            positive = positive.to(device)\n",
        "            negative = negative.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            anchor_out = model(anchor)\n",
        "            positive_out = model(positive)\n",
        "            negative_out = model(negative)\n",
        "\n",
        "            loss = criterion(anchor_out, positive_out, negative_out)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Accuracy and F1\n",
        "            correct, total = compute_accuracy(anchor_out, positive_out, negative_out)\n",
        "            train_correct += correct\n",
        "            train_total += total\n",
        "\n",
        "           # Para F1: predição = 1 se pos < neg senão 0\n",
        "            preds = (F.pairwise_distance(anchor_out, positive_out) < F.pairwise_distance(anchor_out, negative_out)).long().cpu().numpy()\n",
        "            labels = np.ones_like(preds)  # A classe verdadeira é sempre 1 (pos closer)\n",
        "            train_preds.extend(preds)\n",
        "            train_labels.extend(labels)\n",
        "\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        train_accuracy = train_correct / train_total\n",
        "        train_f1 = f1_score(train_labels, train_preds)\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_accuracy)\n",
        "        history['train_f1'].append(train_f1)\n",
        "\n",
        "        print(f'Train Loss: {avg_train_loss:.4f} | Accuracy: {train_accuracy:.4f} | F1: {train_f1:.4f}')\n",
        "\n",
        "        # -------- Validate --------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        val_preds = []\n",
        "        val_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if use_tqdm:\n",
        "                val_loader = tqdm(val_loader, desc='validating')\n",
        "            for anchor, positive, negative in val_loader:\n",
        "                anchor = anchor.to(device)\n",
        "                positive = positive.to(device)\n",
        "                negative = negative.to(device)\n",
        "\n",
        "                anchor_out = model(anchor)\n",
        "                positive_out = model(positive)\n",
        "                negative_out = model(negative)\n",
        "\n",
        "                loss = criterion(anchor_out, positive_out, negative_out)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Accuracy and F1\n",
        "                correct, total = compute_accuracy(anchor_out, positive_out, negative_out)\n",
        "                val_correct += correct\n",
        "                val_total += total\n",
        "\n",
        "                preds = (F.pairwise_distance(anchor_out, positive_out) < F.pairwise_distance(anchor_out, negative_out)).long().cpu().numpy()\n",
        "                labels = np.ones_like(preds)\n",
        "                val_preds.extend(preds)\n",
        "                val_labels.extend(labels)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        val_accuracy = val_correct / val_total\n",
        "        val_f1 = f1_score(val_labels, val_preds)\n",
        "        if scheduler is not None:\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "        history['val_f1'].append(val_f1)\n",
        "\n",
        "        print(f'Val Loss: {avg_val_loss:.4f} | Accuracy: {val_accuracy:.4f} | F1: {val_f1:.4f}')\n",
        "\n",
        "        # -------- Save Best Model --------\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            print(f'Saving best model (Val Loss improved from {best_val_loss:.4f} → {avg_val_loss:.4f})')\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_model_wts, save_path)\n",
        "\n",
        "    print(f'\\n Training complete. Best Val Loss: {best_val_loss:.4f}')\n",
        "\n",
        "    # Carregar e retornar o melhor modelo + histórico de treinamento\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "GNumkWn2vo9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Main"
      ],
      "metadata": {
        "id": "KNYaozmCxHMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 1. Instanciar o modelo\n",
        "model = TripletNetwork(model_name='resnet50', embedding_dim=512)\n",
        "\n",
        "# 2. Defina a função de perda (perda tripla)\n",
        "criterion = nn.TripletMarginLoss(margin=1.0, p=2)\n",
        "\n",
        "# 3. Defina o otimizador\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=4)\n",
        "\n",
        "# 4. Data\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(128, scale=(0.6, 1.0), ratio=(0.75, 1.33)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=20),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.25, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
        "])\n",
        "\n",
        "train_dataset = TripletVGGFaceDataset(\n",
        "    root_dir='/content/drive/MyDrive/Alunos',\n",
        "    transform=train_transform\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "val_dataset = TripletVGGFaceDataset(\n",
        "    root_dir='/content/drive/MyDrive/Alunos',\n",
        "    transform=val_transform\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# 5. Treine e salve o melhor modelo\n",
        "best_model, history = train_triplet_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=3,\n",
        "    device=device,\n",
        "    save_path='melhor_modelo_triplet.pth',\n",
        "    use_tqdm=False,\n",
        "    scheduler=scheduler\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIsttbqtw8mA",
        "outputId": "6a04d7a9-2cf8-4ec4-947c-1ad31aebeb28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 206MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.3705 | Accuracy: 0.8249 | F1: 0.9040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.3249 | Accuracy: 0.8512 | F1: 0.9196\n",
            "Saving best model (Val Loss improved from inf → 0.3249)\n",
            "Epoch 2/3\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2297 | Accuracy: 0.9178 | F1: 0.9571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.3226 | Accuracy: 0.8923 | F1: 0.9431\n",
            "Saving best model (Val Loss improved from 0.3249 → 0.3226)\n",
            "Epoch 3/3\n",
            "------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1332 | Accuracy: 0.9624 | F1: 0.9808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n",
            "/tmp/ipython-input-3199506658.py:35: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  gray_pil = Image.fromarray(gray, mode='L')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Loss: 0.3284 | Accuracy: 0.8462 | F1: 0.9167\n",
            "\n",
            " Training complete. Best Val Loss: 0.3226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curve(history['train_loss'], history['val_loss'])\n",
        "plot_accuracy_curve(history['train_acc'], history['val_acc'])"
      ],
      "metadata": {
        "id": "Ng-ViY5jyvy2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}